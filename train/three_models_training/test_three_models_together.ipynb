{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "app_root = os.path.abspath(os.path.join(project_root, '../../app', '..'))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    sys.path.append(app_root)"
   ],
   "id": "9a2ce9a7fbe79d4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "math_df = pd.read_csv(os.path.join('../datasets_preprocessing/csv_question_files', 'math.csv'))\n",
    "math_df.head(10)"
   ],
   "id": "a79f1abfcc8f4517"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "bio_df = pd.read_csv(os.path.join('../datasets_preprocessing/csv_question_files', 'bio.csv'))\n",
    "bio_df.head(10)"
   ],
   "id": "fbc3008cf20cf411"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "code_df = pd.read_csv(os.path.join('../datasets_preprocessing/csv_question_files', 'code.csv'))\n",
    "code_df.head(10)"
   ],
   "id": "1d18bb6c0b1d9cb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "full_df = pd.concat(\n",
    "\t[\n",
    "\t\tmath_df,\n",
    "\t\tbio_df,\n",
    "\t\tcode_df\n",
    "\t],\n",
    "    ignore_index=True,\n",
    "\taxis=0\n",
    ")\n",
    "\n",
    "full_df = full_df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
    "full_df"
   ],
   "id": "fb7bdf4ac5f1a85b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from train.reporting.model_interface import ModelInterface # IMPORTANT\n",
    "from train.reporting.text_svm_wrapper import TextSVMWrapper # IMPORTANT cannot load models without\n",
    "from typing import Tuple\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def import_model_and_its_test_set(path: str) -> Tuple[ModelInterface, pd.DataFrame]:\n",
    "        with open(path + \"/model.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "\n",
    "        test_set = pd.read_csv(\n",
    "            path + \"/test_set.csv\", index_col=0)\n",
    "        return model, test_set\n",
    "\n",
    "\n",
    "\n",
    "math_model, _ = import_model_and_its_test_set(\"../saved_models/math\")\n",
    "bio_model, _ = import_model_and_its_test_set(\"../saved_models/bio\")\n",
    "code_model, _ = import_model_and_its_test_set(\"../saved_models/code\")"
   ],
   "id": "fc05de9897a7bd54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tutaj wybieramy numer zbioru testowego",
   "id": "73dce15110c38420"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_set_number = 0 # allowed 0 1 2",
   "id": "c83ec1be395620be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df = pd.read_csv(f\"../datasets_preprocessing/test_all_models/test_{test_set_number}.csv\", index_col=0)\n",
    "\n",
    "test_df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
    "test_df"
   ],
   "id": "a9a071ba964aafd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels = test_df  # test_df.merge(full_df.drop(columns=\"tags_str\"), on=\"question\", how=\"left\")\n",
    "\n",
    "test_df_with_labels"
   ],
   "id": "90819662c57a461f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[\"real_class\"] = (test_df_with_labels[\"math\"] * 0 +  test_df_with_labels[\"bio\"] * 1 +\n",
    "                                 test_df_with_labels[\"code\"]\n",
    "                                * 2)\n",
    "test_df_with_labels"
   ],
   "id": "c3ace1b1a31db867"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[\"math_preds\"] = math_model.predict_proba(test_df_with_labels[\"question\"])[:, 1]\n",
    "test_df_with_labels[\"bio_preds\"] = bio_model.predict_proba(test_df_with_labels[\"question\"])[:, 1]\n",
    "test_df_with_labels[\"code_preds\"] = code_model.predict_proba(test_df_with_labels[\"question\"])[:, 1]"
   ],
   "id": "738afcacb6258871"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_df_with_labels",
   "id": "58af298ad1b4e164"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "cols = ['math_preds', 'bio_preds', 'code_preds']\n",
    "\n",
    "max_values = test_df_with_labels[cols].max(axis=1)\n",
    "max_names = test_df_with_labels[cols].idxmax(axis=1)\n",
    "\n",
    "class_mapping = {'math_preds': 0, 'bio_preds': 1, 'code_preds': 2}\n",
    "\n",
    "predicted_class = max_names.map(class_mapping)\n",
    "\n",
    "test_df_with_labels['predicted_class'] = np.where(max_values > 0.5, predicted_class, -1)\n",
    "test_df_with_labels\n"
   ],
   "id": "eb881a262ed65fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "accuracy_score(test_df_with_labels[\"real_class\"], test_df_with_labels[\"predicted_class\"])"
   ],
   "id": "2ab88b6125f399b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sample for analyzing errors based on wrong label or ambiguity",
   "id": "619fbda9f99ab7d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "wrong_sample = test_df_with_labels[test_df_with_labels['real_class'] != test_df_with_labels['predicted_class']].sample(n=10)\n",
    "\n",
    "wrong_sample"
   ],
   "id": "deccb59fcbb741a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for question in wrong_sample['question']:\n",
    "\tprint(question)\n"
   ],
   "id": "7bc739047a303ad4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Other models winning over wrong code model test",
   "id": "49e7c956300ff0dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] == test_df_with_labels['predicted_class']) & # But other\n",
    "                    # models overcame te wrong one which resulted in a correct prediction\n",
    "                    (test_df_with_labels['real_class'] != 2) & # but real_class is not code\n",
    "                    (test_df_with_labels['code_preds'] >= 0.5)] # would predict code"
   ],
   "id": "cec94cb02c9136b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] != 2) &\n",
    "                    (test_df_with_labels['code_preds'] >= 0.5)]"
   ],
   "id": "23fda756d2570e2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Other models winning over wrong math model test",
   "id": "67183fea2f9fbf5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] == test_df_with_labels['predicted_class']) &  # But other\n",
    "                    # models overcame te wrong one which resulted in a correct prediction\n",
    "                    (test_df_with_labels['real_class'] != 0) &  # but real_class is not math\n",
    "                    (test_df_with_labels['math_preds'] >= 0.5)]  # would predict math"
   ],
   "id": "caf85ab70975af11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] != 0) &\n",
    "                    (test_df_with_labels['math_preds'] >= 0.5)] # all wrong predictions"
   ],
   "id": "a975d81f14ed615e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Other models winning over wrong bio model test",
   "id": "1f314e5ebbd0eb9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] == test_df_with_labels['predicted_class']) &  # But other\n",
    "                    # models overcame te wrong one which resulted in a correct prediction\n",
    "                    (test_df_with_labels['real_class'] != 1) &  # but real_class is not bio\n",
    "                    (test_df_with_labels['bio_preds'] >= 0.5)]  # would predict bio"
   ],
   "id": "7e4abb400ba31be5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] != 1) &\n",
    "                    (test_df_with_labels['bio_preds'] >= 0.5)] # all wrong predictions"
   ],
   "id": "323ea5db72aca24b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Other models winning over right code model test",
   "id": "f515a6bfd7eeccfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] != test_df_with_labels['predicted_class']) & # But other\n",
    "                    # models overcame the right one which resulted in a wrong prediction\n",
    "                    (test_df_with_labels['real_class'] == 2) & # real_class is code\n",
    "                    (test_df_with_labels['code_preds'] >= 0.5)] # would predict code"
   ],
   "id": "1a968b9e9bf096aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] == 2) &\n",
    "                    (test_df_with_labels['code_preds'] >= 0.5)]"
   ],
   "id": "72ed08e91cb3214"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Other models winning over right math model",
   "id": "5fc155e7ab57d8a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] != test_df_with_labels['predicted_class']) & # But other\n",
    "                    # models overcame the right one which resulted in a wrong prediction\n",
    "                    (test_df_with_labels['real_class'] == 0) & # real_class is math\n",
    "                    (test_df_with_labels['math_preds'] >= 0.5)] # would predict math"
   ],
   "id": "bf98a3cb0c3a479a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] == 0) &\n",
    "                    (test_df_with_labels['math_preds'] >= 0.5)]"
   ],
   "id": "15da505b467aea40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Other models winning over right bio model",
   "id": "15e7fd2710bbf220"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] != test_df_with_labels['predicted_class']) & # But other\n",
    "                    # models overcame the right one which resulted in a wrong prediction\n",
    "                    (test_df_with_labels['real_class'] == 1) & # real_class is bio\n",
    "                    (test_df_with_labels['bio_preds'] >= 0.5)] # would predict bio"
   ],
   "id": "832fa1b10b170288"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] == 1) &\n",
    "                    (test_df_with_labels['bio_preds'] >= 0.5)]"
   ],
   "id": "3be2fce2cb2d8bc7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
