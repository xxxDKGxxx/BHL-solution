{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "app_root = os.path.abspath(os.path.join(project_root, '../../app', '..'))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    sys.path.append(app_root)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from train.datasets_preprocessing.datasets_preprocessing import load_json_data, make_pipeline\n",
    "import pandas as pd\n",
    "\n",
    "math_pipeline = make_pipeline('math')\n",
    "\n",
    "X_json_raw = load_json_data('../datasets_preprocessing/datasets/math')\n",
    "math_pipeline.fit_transform(X_json_raw)\n",
    "\n",
    "math_df = pd.read_csv(os.path.join('../datasets_preprocessing/csv_question_files', 'math.csv'))\n",
    "math_df.head(10)"
   ],
   "id": "991c077de874cc2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bio_pipeline = make_pipeline('bio')\n",
    "\n",
    "X_json_raw = load_json_data('../datasets_preprocessing/datasets/bio')\n",
    "bio_pipeline.fit_transform(X_json_raw)\n",
    "\n",
    "bio_df = pd.read_csv(os.path.join('../datasets_preprocessing/csv_question_files', 'bio.csv'))\n",
    "bio_df.head(10)"
   ],
   "id": "614cc2b85e2e413",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "code_pipeline = make_pipeline('code')\n",
    "\n",
    "X_json_raw = load_json_data('../datasets_preprocessing/datasets/code')\n",
    "code_pipeline.fit_transform(X_json_raw)\n",
    "\n",
    "code_df = pd.read_csv(os.path.join('../datasets_preprocessing/csv_question_files', 'code.csv'))\n",
    "code_df.head(10)"
   ],
   "id": "9b858933af03faee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "full_df = pd.concat(\n",
    "\t[\n",
    "\t\tmath_df,\n",
    "\t\tbio_df,\n",
    "\t\tcode_df\n",
    "\t],\n",
    "    ignore_index=True,\n",
    "\taxis=0\n",
    ")\n",
    "\n",
    "full_df = full_df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
    "full_df"
   ],
   "id": "334959be2c25435d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from train.reporting.model_interface import ModelInterface # IMPORTANT\n",
    "from train.reporting.text_svm_wrapper import TextSVMWrapper # IMPORTANT cannot load models without\n",
    "from typing import Tuple\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def import_model_and_its_test_set(path: str) -> Tuple[ModelInterface, pd.DataFrame]:\n",
    "        with open(path + \"/model.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "\n",
    "        test_set = pd.read_csv(\n",
    "            path + \"/test_set.csv\", index_col=0)\n",
    "        return model, test_set\n",
    "\n",
    "\n",
    "\n",
    "math_model, _ = import_model_and_its_test_set(\"../saved_models/math\")\n",
    "bio_model, _ = import_model_and_its_test_set(\"../saved_models/bio\")\n",
    "code_model, _ = import_model_and_its_test_set(\"../saved_models/code\")"
   ],
   "id": "1ac1a23b148e36d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tutaj wybieramy numer zbioru testowego",
   "id": "ef676790faba1342"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_set_number = 0 # allowed 0 1 2",
   "id": "671a1a1323a91851",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv(f\"../datasets_preprocessing/test_all_models/test_{test_set_number}.csv\", index_col=0)\n",
    "\n",
    "test_df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
    "test_df"
   ],
   "id": "cb168816a25d21b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels = test_df  # test_df.merge(full_df.drop(columns=\"tags_str\"), on=\"question\", how=\"left\")\n",
    "\n",
    "test_df_with_labels"
   ],
   "id": "d049c520260b1882",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[\"real_class\"] = (test_df_with_labels[\"math\"] * 0 +  test_df_with_labels[\"bio\"] * 1 +\n",
    "                                 test_df_with_labels[\"code\"]\n",
    "                                * 2)\n",
    "test_df_with_labels"
   ],
   "id": "5313a22db126835",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[\"math_preds\"] = math_model.predict_proba(test_df_with_labels[\"question\"])[:, 1]\n",
    "test_df_with_labels[\"bio_preds\"] = bio_model.predict_proba(test_df_with_labels[\"question\"])[:, 1]\n",
    "test_df_with_labels[\"code_preds\"] = code_model.predict_proba(test_df_with_labels[\"question\"])[:, 1]"
   ],
   "id": "960646fe54c71a81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_df_with_labels",
   "id": "40aef67f54c2ba40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "cols = ['math_preds', 'bio_preds', 'code_preds']\n",
    "\n",
    "max_values = test_df_with_labels[cols].max(axis=1)\n",
    "max_names = test_df_with_labels[cols].idxmax(axis=1)\n",
    "\n",
    "class_mapping = {'math_preds': 0, 'bio_preds': 1, 'code_preds': 2}\n",
    "\n",
    "predicted_class = max_names.map(class_mapping)\n",
    "\n",
    "test_df_with_labels['predicted_class'] = np.where(max_values > 0.5, predicted_class, -1)\n",
    "test_df_with_labels\n"
   ],
   "id": "e81c1cfef9886e94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "accuracy_score(test_df_with_labels[\"real_class\"], test_df_with_labels[\"predicted_class\"])"
   ],
   "id": "d9d1863f06fc03b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sample for analyzing errors based on wrong label or ambiguity",
   "id": "15ed5d37e873a218"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "wrong_sample = test_df_with_labels[test_df_with_labels['real_class'] != test_df_with_labels['predicted_class']].sample(n=10)\n",
    "\n",
    "wrong_sample"
   ],
   "id": "df86f41c84831cba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for question in wrong_sample['question']:\n",
    "\tprint(question)\n"
   ],
   "id": "962ad9241ff26ede",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Other models winning over wrong code model test",
   "id": "7bdd40de57ffaba2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] == test_df_with_labels['predicted_class']) & # But other\n",
    "                    # models overcame te wrong one which resulted in a correct prediction\n",
    "                    (test_df_with_labels['real_class'] != 2) & # but real_class is not code\n",
    "                    (test_df_with_labels['code_preds'] >= 0.5)] # would predict code"
   ],
   "id": "686fbd69f4748cdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] != 2) &\n",
    "                    (test_df_with_labels['code_preds'] >= 0.5)]"
   ],
   "id": "16d97b312f04cdec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Other models winning over wrong math model test",
   "id": "4b7df29a2cb81db1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] == test_df_with_labels['predicted_class']) &  # But other\n",
    "                    # models overcame te wrong one which resulted in a correct prediction\n",
    "                    (test_df_with_labels['real_class'] != 0) &  # but real_class is not math\n",
    "                    (test_df_with_labels['math_preds'] >= 0.5)]  # would predict math"
   ],
   "id": "3fc7fe12a409b740",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] != 0) &\n",
    "                    (test_df_with_labels['math_preds'] >= 0.5)] # all wrong predictions"
   ],
   "id": "d6589e5548d3d24c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Other models winning over wrong bio model test",
   "id": "7809e081a3f29e27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] == test_df_with_labels['predicted_class']) &  # But other\n",
    "                    # models overcame te wrong one which resulted in a correct prediction\n",
    "                    (test_df_with_labels['real_class'] != 1) &  # but real_class is not bio\n",
    "                    (test_df_with_labels['bio_preds'] >= 0.5)]  # would predict bio"
   ],
   "id": "848408a8b60257b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] != 1) &\n",
    "                    (test_df_with_labels['bio_preds'] >= 0.5)] # all wrong predictions"
   ],
   "id": "4a8666e8b5c1a93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Other models winning over right code model test",
   "id": "3e5c13414e7b0ebe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] != test_df_with_labels['predicted_class']) & # But other\n",
    "                    # models overcame the right one which resulted in a wrong prediction\n",
    "                    (test_df_with_labels['real_class'] == 2) & # real_class is code\n",
    "                    (test_df_with_labels['code_preds'] >= 0.5)] # would predict code"
   ],
   "id": "c1a382cd4c029637",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] == 2) &\n",
    "                    (test_df_with_labels['code_preds'] >= 0.5)]"
   ],
   "id": "ddce5833d1fa1d2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Other models winning over right math model",
   "id": "34dd2ec24e489875"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] != test_df_with_labels['predicted_class']) & # But other\n",
    "                    # models overcame the right one which resulted in a wrong prediction\n",
    "                    (test_df_with_labels['real_class'] == 0) & # real_class is math\n",
    "                    (test_df_with_labels['math_preds'] >= 0.5)] # would predict math"
   ],
   "id": "248261870a3c67dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] == 0) &\n",
    "                    (test_df_with_labels['math_preds'] >= 0.5)]"
   ],
   "id": "1e74e15957d13596",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Other models winning over right bio model",
   "id": "d74ea163c1ff5f3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] != test_df_with_labels['predicted_class']) & # But other\n",
    "                    # models overcame the right one which resulted in a wrong prediction\n",
    "                    (test_df_with_labels['real_class'] == 1) & # real_class is bio\n",
    "                    (test_df_with_labels['bio_preds'] >= 0.5)] # would predict bio"
   ],
   "id": "1b460b7d1b72f406",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df_with_labels[(test_df_with_labels['real_class'] == 1) &\n",
    "                    (test_df_with_labels['bio_preds'] >= 0.5)]"
   ],
   "id": "ae23ff56435868a0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
